from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from govdocs_api.utilities.pdf_utilities import render_pdf_to_base64png, total_pages
import os
import torch
import json
import gc
from io import BytesIO
from PIL import Image
import base64
# import hashlib
# import atexit
from fastapi import FastAPI, APIRouter, HTTPException, BackgroundTasks
from contextlib import asynccontextmanager
from functools import partial, cache
import time
# import httpx
# import asyncio
# from pypdf import PdfReader
# from tqdm import tqdm
# from dataclasses import dataclass
# from urllib.parse import urlparse
# from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
# from concurrent.futures.process import BrokenProcessPool
from typing import List, Dict, Any, Optional, Tuple
# import multiprocessing
# import logging
# import re

# from olmocr.check import (
#     check_poppler_version,
#     check_sglang_version,
#     check_torch_gpu_available,
# )
# from olmocr.filter.filter import Language, PdfFilter
# from olmocr.metrics import MetricsKeeper, WorkerTracker
from olmocr.prompts import PageResponse, build_finetuning_prompt
from olmocr.prompts.anchor import get_anchor_text
# from olmocr.s3_utils import (
#     download_zstd_csv,
#     expand_s3_glob,
#     get_s3_bytes,
#     get_s3_bytes_with_backoff,
#     parse_s3_path,
# )
# from olmocr.version import VERSION
# from olmocr.work_queue import LocalWorkQueue, S3WorkQueue, WorkQueue
from govdocs_api.supabase.db_functions import supabase
import io


model = None
processor = None
# metrics = MetricsKeeper(window=300)  # 5 minute window
# tracker = WorkerTracker()

# # Process pool for CPU-bound operations
# process_pool = ProcessPoolExecutor(
#     max_workers=min(multiprocessing.cpu_count() // 2 + 1, 16),
#     mp_context=multiprocessing.get_context("spawn")
# )

# # Constants
# SGLANG_SERVER_PORT = 30024  # Adjust as needed

# sglang_logger = logging.getLogger("sglang")
# sglang_logger.setLevel(logging.INFO)
# # Initialize logger
# logger = logging.getLogger("olm-ocr-api")
# logger.setLevel(logging.DEBUG)

# file_handler = logging.FileHandler("olm-ocr-api-debug.log", mode="a")
# file_handler.setLevel(logging.DEBUG)
# file_handler.setFormatter(logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s"))

# console_handler = logging.StreamHandler()
# console_handler.setLevel(logging.INFO)
# console_handler.setFormatter(logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s"))

# logger.addHandler(file_handler)
# logger.addHandler(console_handler)

# # Metrics tracking
# class MetricsKeeper:
#     def __init__(self, window=300):
#         self.window = window
#         self.metrics = {}
#         self.timestamps = []
    
#     def add_metrics(self, **kwargs):
#         timestamp = time.time()
#         self.timestamps.append(timestamp)
#         for key, value in kwargs.items():
#             if key not in self.metrics:
#                 self.metrics[key] = []
#             self.metrics[key].append(value)
        
#         # Clean old metrics
#         self._clean_old_metrics()
    
#     def _clean_old_metrics(self):
#         if not self.timestamps:
#             return
        
#         cutoff = time.time() - self.window
#         while self.timestamps and self.timestamps[0] < cutoff:
#             self.timestamps.pop(0)
#             for key in self.metrics:
#                 if self.metrics[key]:
#                     self.metrics[key].pop(0)
    
#     def get_summary(self):
#         self._clean_old_metrics()
#         summary = {}
#         for key, values in self.metrics.items():
#             if values:
#                 summary[key] = {
#                     "total": sum(values),
#                     "count": len(values),
#                     "avg": sum(values) / len(values),
#                     "max": max(values),
#                     "min": min(values)
#                 }
#         return summary

# class WorkerTracker:
#     def __init__(self):
#         self.status = {}
#         self.started_at = {}
#         self._lock = asyncio.Lock()
    
#     async def track_work(self, worker_id, work_id, status):
#         async with self._lock:
#             self.status[work_id] = status
#             if status == "started":
#                 self.started_at[work_id] = time.time()
    
#     def get_status(self):
#         active_workers = {}
#         for work_id, status in self.status.items():
#             if status == "started":
#                 elapsed = time.time() - self.started_at.get(work_id, time.time())
#                 active_workers[work_id] = {
#                     "status": status,
#                     "elapsed_seconds": elapsed
#                 }
#         return active_workers

# @dataclass(frozen=True)
# class PageResult:
#     s3_path: str
#     page_num: int
#     response: PageResponse

#     input_tokens: int
#     output_tokens: int
#     is_fallback: bool

# async def sglang_server_task(semaphore):
#     #model_name_or_path = args.model
#     #model_name_or_path = '/local/home/hfurquan/myProjects/Leaderboard/cache/models--allenai--olmOCR-7B-0225-preview'
#     model_name_or_path = "allenai/olmOCR-7B-0225-preview"

#     # Check GPU memory, lower mem devices need a bit less KV cache space because the VLM takes additional memory
#     gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert to GB
#     mem_fraction_arg = ["--mem-fraction-static", "0.80"] if gpu_memory < 60 else []

#     cmd = [
#         "python3",
#         "-m",
#         "sglang.launch_server",
#         "--model-path",
#         model_name_or_path,
#         "--chat-template",
#         "qwen2-vl",
#         "--port",
#         str(SGLANG_SERVER_PORT),
#         "--log-level-http",
#         "warning",
#         "--device",
#         "cuda",
#         "--base-gpu-id",
#         "0"
#     ]
#     cmd.extend(mem_fraction_arg)

#     proc = await asyncio.create_subprocess_exec(
#         *cmd,
#         stdout=asyncio.subprocess.PIPE,
#         stderr=asyncio.subprocess.PIPE,
#     )

#     # Ensure the subprocess is terminated on exit
#     def _kill_proc():
#         proc.terminate()

#     atexit.register(_kill_proc)

#     # Shared variables between tasks
#     last_running_req, last_queue_req = 0, 0
#     server_printed_ready_message = False
#     last_semaphore_release = time.time()

#     async def process_line(line):
#         nonlocal last_running_req, last_queue_req, last_semaphore_release, server_printed_ready_message
#         sglang_logger.info(line)

#         # if the server hasn't initialized yet, log all the lines to the main logger also, so that the user
#         # can see any warnings/errors more easily
#         if not server_printed_ready_message:
#             logger.info(line)

#         if "Detected errors during sampling" in line:
#             logger.error("Cannot continue, sampling errors detected, model is probably corrupt")
#             sys.exit(1)

#         # TODO, need to trace down this issue in sglang itself, but it will otherwise cause the server to lock up
#         if "IndexError: list index out of range" in line:
#             logger.error("IndexError in model, restarting server")
#             proc.terminate()

#         if not server_printed_ready_message and "The server is fired up and ready to roll!" in line:
#             server_printed_ready_message = True
#             last_semaphore_release = time.time()

#         match = re.search(r"#running-req: (\d+)", line)
#         if match:
#             last_running_req = int(match.group(1))

#         match = re.search(r"#queue-req: (\d+)", line)
#         if match:
#             last_queue_req = int(match.group(1))
#             logger.info(f"sglang running req: {last_running_req} queue req: {last_queue_req}")

#     async def read_stream(stream):
#         while True:
#             line = await stream.readline()
#             if not line:
#                 break
#             try:
#                 line = line.decode("utf-8").rstrip()
#                 await process_line(line)
#             except Exception as ex:
#                 logger.warning(f"Got {ex} when reading log line from inference server, skipping")

#     async def timeout_task():
#         nonlocal last_running_req, last_queue_req, last_semaphore_release
#         try:
#             while True:
#                 await asyncio.sleep(1)
#                 if server_printed_ready_message and last_queue_req == 0 and time.time() - last_semaphore_release > 30 and semaphore.locked():
#                     semaphore.release()
#                     last_semaphore_release = time.time()
#                     logger.info("Semaphore released, allowing a worker to proceed.")
#         except asyncio.CancelledError:
#             pass  # Clean up if the task is cancelled

#     # Start tasks to read stdout, stderr, and handle timeout logic
#     stdout_task = asyncio.create_task(read_stream(proc.stdout))
#     stderr_task = asyncio.create_task(read_stream(proc.stderr))
#     timeout_task = asyncio.create_task(timeout_task())

#     try:
#         await proc.wait()
#     except asyncio.CancelledError:
#         logger.info("Got cancellation request for SGLang server")
#         proc.terminate()
#         raise

#     timeout_task.cancel()
#     await asyncio.gather(stdout_task, stderr_task, timeout_task, return_exceptions=True)


# async def sglang_server_ready():
#     max_attempts = 300
#     delay_sec = 1
#     url = f"http://localhost:{SGLANG_SERVER_PORT}/v1/models"

#     for attempt in range(1, max_attempts + 1):
#         try:
#             async with httpx.AsyncClient() as session:
#                 response = await session.get(url)

#                 if response.status_code == 200:
#                     logger.info("sglang server is ready.")
#                     return
#                 else:
#                     logger.info(f"Attempt {attempt}: Unexpected status code {response.status_code}")
#         except Exception:
#             logger.warning(f"Attempt {attempt}: Please wait for sglang server to become ready...")

#         await asyncio.sleep(delay_sec)

#     raise Exception("sglang server did not become ready after waiting.")


# # Simple HTTP client for SGLang server communication
# async def apost(url, json_data):
#     parsed_url = urlparse(url)
#     host = parsed_url.hostname
#     port = parsed_url.port or 80
#     path = parsed_url.path or "/"
    
#     request_start = time.perf_counter()
#     writer = None
#     try:
#         reader, writer = await asyncio.open_connection(host, port)
        
#         json_payload = json.dumps(json_data)
#         request = (
#             f"POST {path} HTTP/1.1\r\n"
#             f"Host: {host}\r\n"
#             f"Content-Type: application/json\r\n"
#             f"Content-Length: {len(json_payload)}\r\n"
#             f"Connection: close\r\n\r\n"
#             f"{json_payload}"
#         )
#         writer.write(request.encode())
#         await writer.drain()
        
#         # Read status line
#         status_line = await reader.readline()
#         if not status_line:
#             raise ConnectionError("No response from server")
        
#         status_parts = status_line.decode().strip().split(" ", 2)
#         if len(status_parts) < 2:
#             raise ValueError(f"Malformed status line: {status_line.decode().strip()}")
        
#         status_code = int(status_parts[1])
        
#         # Read headers
#         headers = {}
#         while True:
#             line = await reader.readline()
#             if line in (b"\r\n", b"\n", b""):
#                 break
#             key, _, value = line.decode().partition(":")
#             headers[key.strip().lower()] = value.strip()
        
#         # Read response body
#         if "content-length" in headers:
#             body_length = int(headers["content-length"])
#             response_body = await reader.readexactly(body_length)
#         else:
#             chunks = []
#             while True:
#                 chunk = await reader.read(4096)
#                 if not chunk:
#                     break
#                 chunks.append(chunk)
#             response_body = b''.join(chunks)
        
#         request_end = time.perf_counter()
#         logger.debug(f"SGLang request completed in {request_end - request_start:.2f}s")
        
#         return status_code, response_body
#     except Exception as e:
#         request_end = time.perf_counter()
#         logger.warning(f"SGLang request failed in {request_end - request_start:.2f}s: {str(e)}")
#         raise
#     finally:
#         if writer is not None:
#             try:
#                 writer.close()
#                 await writer.wait_closed()
#             except:
#                 pass

# async def build_page_query(local_pdf_path: str, page: int, target_longest_image_dim: int, 
#                           target_anchor_text_len: int, image_rotation: int = 0) -> dict:
#     query_start = time.perf_counter()
#     MAX_TOKENS = 3000
#     assert image_rotation in [0, 90, 180, 270], "Invalid image rotation provided"
    
#     # Run these operations concurrently
#     render_start = time.perf_counter()
#     image_base64_task = asyncio.create_task(
#         asyncio.to_thread(render_pdf_to_base64png, local_pdf_path, page, 
#                          target_longest_image_dim=target_longest_image_dim)
#     )
    
#     # Get anchor text (CPU-bound operation)
#     anchor_start = time.perf_counter()
#     loop = asyncio.get_running_loop()
#     anchor_text_task = asyncio.create_task(
#         loop.run_in_executor(
#             process_pool, 
#             partial(get_anchor_text, pdf_engine="pdfreport", target_length=target_anchor_text_len), 
#             local_pdf_path, page
#         )
#     )
    
#     # Wait for both operations to complete
#     image_base64, anchor_text = await asyncio.gather(image_base64_task, anchor_text_task)
#     render_end = time.perf_counter()
#     anchor_end = time.perf_counter()
    
#     # Handle image rotation if needed
#     if image_rotation != 0:
#         rotation_start = time.perf_counter()
#         image_bytes = base64.b64decode(image_base64)
#         with Image.open(BytesIO(image_bytes)) as img:
#             rotated_img = img.rotate(-image_rotation, expand=True)
#             buffered = BytesIO()
#             rotated_img.save(buffered, format="PNG")
        
#         image_base64 = base64.b64encode(buffered.getvalue()).decode("utf-8")
#         rotation_end = time.perf_counter()
    
#     # Build the query
#     prompt = build_finetuning_prompt(anchor_text)
#     query = {
#         "model": "Qwen/Qwen2-VL-7B-Instruct",
#         "messages": [
#             {
#                 "role": "user",
#                 "content": [
#                     {"type": "text", "text": prompt},
#                     {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_base64}"}},
#                 ],
#             }
#         ],
#         "max_tokens": MAX_TOKENS,
#         "temperature": 0.8, 
#     }
    
#     query_end = time.perf_counter()
    
#     performance = {
#         "render_time": render_end - render_start,
#         "anchor_time": anchor_end - anchor_start,
#         "rotation_time": (rotation_end - rotation_start) if image_rotation != 0 else 0,
#         "total_prep_time": query_end - query_start
#     }
    
#     return query, performance

# async def process_page(worker_id: int, pdf_path: str, page_num: int, 
#                      target_longest_image_dim: int = 256, 
#                      target_anchor_text_len: int = 4000,
#                      max_retries: int = 5) -> PageResult:
#     """Process a single page using SGLang with advanced error handling and retries."""
#     COMPLETION_URL = f"http://localhost:{SGLANG_SERVER_PORT}/v1/chat/completions"
#     TEMPERATURE_BY_ATTEMPT = [0.8, 0.9]
    
#     exponential_backoffs = 0
#     local_anchor_text_len = target_anchor_text_len
#     local_image_rotation = 0
#     attempt = 0
#     page_start = time.perf_counter()
    
#     await tracker.track_work(worker_id, f"{pdf_path}-{page_num}", "started")
    
#     performance_metrics = {
#         "attempts": 0,
#         "backoffs": 0
#     }
    
#     while attempt < max_retries:
#         attempt_start = time.perf_counter()
        
#         try:
#             # Build the query for this page
#             query, prep_performance = await build_page_query(
#                 pdf_path, page_num, 
#                 target_longest_image_dim, 
#                 local_anchor_text_len,
#                 image_rotation=local_image_rotation
#             )
            
            
#             performance_metrics.update(prep_performance)
#             logger.info(f"Built page query for {pdf_path} page {page_num+1}, attempt {attempt+1}")
            
#             # Send request to SGLang server
#             sgapi_start = time.perf_counter()
#             status_code, response_body = await apost(COMPLETION_URL, json_data=query)
#             sgapi_end = time.perf_counter()
#             performance_metrics["sglang_time"] = sgapi_end - sgapi_start
            
#             # Handle HTTP errors
#             if status_code == 400:
#                 raise ValueError(f"BadRequestError from server: {response_body}")
#             elif status_code == 500:
#                 raise ValueError(f"InternalServerError from server: {response_body}")
#             elif status_code != 200:
#                 raise ValueError(f"Error http status {status_code}")
            
#             # Parse the response
#             base_response_data = json.loads(response_body)
            
#             # Check if we exceeded token limits
#             if base_response_data["usage"]["total_tokens"] > 8000:  # Adjust as needed
#                 local_anchor_text_len = max(1, local_anchor_text_len // 2)
#                 logger.info(f"Reducing anchor text len to {local_anchor_text_len} for {pdf_path}-{page_num}")
#                 raise ValueError("Response exceeded model_max_context, cannot use this response")
            
#             # Track metrics
#             metrics.add_metrics(
#                 sglang_input_tokens=base_response_data["usage"].get("prompt_tokens", 0),
#                 sglang_output_tokens=base_response_data["usage"].get("completion_tokens", 0),
#                 sglang_latency=(sgapi_end - sgapi_start) * 1000,  # ms
#             )
            
#             # Extract and parse model response
#             model_response_json = json.loads(base_response_data["choices"][0]["message"]["content"])
            
#             # Check if page rotation is needed
#             if model_response_json.get("is_rotation_valid") is False and attempt < max_retries - 1:
#                 rotation_correction = model_response_json.get("rotation_correction", 0)
#                 logger.info(f"Invalid page rotation for {pdf_path}-{page_num}, attempt {attempt+1}. "
#                            f"Retrying with {rotation_correction} rotation.")
#                 local_image_rotation = rotation_correction
#                 raise ValueError(f"Invalid page rotation for {pdf_path}-{page_num}")
            
#             # Success - mark task as complete
#             await tracker.track_work(worker_id, f"{pdf_path}-{page_num}", "finished")
            
#             page_end = time.perf_counter()
#             performance_metrics.update({
#                 "total_time": page_end - page_start,
#                 "attempts": attempt + 1
#             })
            
#             return PageResult(
#                 page_number=page_num + 1,  # Convert to 1-indexed for user
#                 text=model_response_json.get("natural_text", ""),
#                 input_tokens=base_response_data["usage"].get("prompt_tokens", 0),
#                 output_tokens=base_response_data["usage"].get("completion_tokens", 0),
#                 is_fallback=False,
#                 performance=performance_metrics,
#                 metadata={
#                     "primary_language": model_response_json.get("primary_language", "eng"),
#                     "is_table": model_response_json.get("is_table", False),
#                     "is_diagram": model_response_json.get("is_diagram", False),
#                     "rotation": model_response_json.get("rotation_correction", 0)
#                 }
#             )
            
#         except (ConnectionError, OSError, asyncio.TimeoutError) as e:
#             # Network-related errors - use exponential backoff
#             logger.warning(f"Network error on attempt {attempt+1} for {pdf_path}-{page_num}: {type(e).__name__} - {e}")
            
#             sleep_delay = 10 * (2 ** exponential_backoffs)
#             exponential_backoffs += 1
#             performance_metrics["backoffs"] = exponential_backoffs
            
#             logger.info(f"Sleeping for {sleep_delay}s on {pdf_path}-{page_num} to allow server recovery")
#             await asyncio.sleep(sleep_delay)
            
#         except asyncio.CancelledError:
#             # Task was cancelled - propagate the cancellation
#             logger.info(f"Process page {pdf_path}-{page_num} cancelled")
#             await tracker.track_work(worker_id, f"{pdf_path}-{page_num}", "cancelled")
#             raise
            
#         except json.JSONDecodeError as e:
#             # JSON parsing error - retry with next temperature
#             logger.warning(f"JSON decode error on attempt {attempt+1} for {pdf_path}-{page_num}: {e}")
#             attempt += 1
            
#         except ValueError as e:
#             # Value error (includes rotation issues) - retry
#             logger.warning(f"ValueError on attempt {attempt+1} for {pdf_path}-{page_num}: {type(e).__name__} - {e}")
#             attempt += 1
            
#         except Exception as e:
#             # Unexpected error - log and retry
#             logger.exception(f"Unexpected error on attempt {attempt+1} for {pdf_path}-{page_num}: {type(e).__name__} - {e}")
#             attempt += 1
        
#         attempt_end = time.perf_counter()
#         logger.debug(f"Attempt {attempt} for {pdf_path}-{page_num} took {attempt_end - attempt_start:.2f}s")
    
#     # All retries failed - fall back to direct model processing
#     logger.error(f"Failed to process {pdf_path}-{page_num} after {max_retries} attempts, using fallback.")
#     await tracker.track_work(worker_id, f"{pdf_path}-{page_num}", "errored")
    
#     # Use direct model if SGLang failed
#     fallback_start = time.perf_counter()
#     try:
#         # Get anchor text as fallback text
#         anchor_text = await asyncio.to_thread(
#             get_anchor_text, pdf_path, page_num, pdf_engine="pdftotext"
#         )
        
#         fallback_end = time.perf_counter()
#         performance_metrics["fallback_time"] = fallback_end - fallback_start
        
#         return PageResult(
#             page_number=page_num + 1,
#             text=anchor_text,
#             input_tokens=0,
#             output_tokens=0,
#             is_fallback=True,
#             performance=performance_metrics,
#             metadata={
#                 "primary_language": None,
#                 "is_table": False,
#                 "is_diagram": False,
#                 "rotation": 0
#             }
#         )
#     except Exception as e:
#         logger.exception(f"Even fallback processing failed for {pdf_path}-{page_num}: {e}")
#         return PageResult(
#             page_number=page_num + 1,
#             text="ERROR: Failed to extract text from this page.",
#             is_fallback=True,
#             performance=performance_metrics
#         )

# # Process PDFs in parallel with controlled concurrency
# async def process_pdf(pdf_path: str, 
#                       first_page: int = 1, 
#                       last_page: int = None,
#                       max_concurrent_pages: int = 4,
#                       **kwargs) -> List[PageResult]:
#     """Process a PDF file using parallel page processing with SGLang."""
#     pdf_start = time.perf_counter()
    
#     try:
#         # Get total page count
#         page_count_start = time.perf_counter()
#         num_pages = total_pages(pdf_path)
#         page_count_end = time.perf_counter()
        
#         logger.info(f"Processing PDF {pdf_path} with {num_pages} pages")
        
#         # Validate and adjust page range
#         first_page = max(1, first_page)
#         if last_page is None or last_page > num_pages:
#             last_page = num_pages
        
#         # Convert to 0-based indexing for internal use
#         pages_to_process = range(first_page - 1, last_page)
#         total_pages_to_process = len(pages_to_process)
        
#         # Use semaphore to limit concurrent page processing
#         semaphore = asyncio.Semaphore(max_concurrent_pages)
        
#         async def process_with_semaphore(worker_id, page_num):
#             async with semaphore:
#                 return await process_page(
#                     worker_id=worker_id,
#                     pdf_path=pdf_path,
#                     page_num=page_num,
#                     **kwargs
#                 )
        
#         # Create tasks for all pages
#         tasks = []
#         for i, page_num in enumerate(pages_to_process):
#             worker_id = i % max_concurrent_pages  # Assign worker IDs
#             tasks.append(asyncio.create_task(
#                 process_with_semaphore(worker_id, page_num)
#             ))
        
#         # Wait for all tasks to complete
#         page_results = await asyncio.gather(*tasks, return_exceptions=True)
        
#         # Handle any exceptions in results
#         processed_results = []
#         for i, result in enumerate(page_results):
#             if isinstance(result, Exception):
#                 logger.error(f"Error processing page {pages_to_process[i]+1}: {result}")
#                 # Create error result
#                 processed_results.append(PageResult(
#                     page_number=pages_to_process[i] + 1,
#                     text=f"ERROR: Failed to process page: {str(result)}",
#                     is_fallback=True
#                 ))
#             else:
#                 processed_results.append(result)
        
#         # Sort by page number
#         processed_results.sort(key=lambda x: x.page_number)
        
#         # Calculate success rate
#         fallback_pages = sum(1 for r in processed_results if r.is_fallback)
#         success_rate = (total_pages_to_process - fallback_pages) / total_pages_to_process if total_pages_to_process > 0 else 0
        
#         pdf_end = time.perf_counter()
#         logger.info(f"PDF {pdf_path} processed in {pdf_end - pdf_start:.2f}s with {success_rate:.1%} success rate")
        
#         return processed_results
        
#     except Exception as e:
#         logger.exception(f"Failed to process PDF {pdf_path}: {e}")
#         raise HTTPException(status_code=500, detail=f"PDF processing failed: {str(e)}")



#,cache_dir="/local/home/hfurquan/myProjects/Leaderboard/cache"

@asynccontextmanager
async def lifespan(app: FastAPI):
    global model
    global processor
    model = Qwen2VLForConditionalGeneration.from_pretrained(
        "allenai/olmOCR-7B-0225-preview", 
        torch_dtype=torch.bfloat16
    ).eval()
    processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-7B-Instruct")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)


    # Create a semaphore to control worker access
    # We only allow one worker to move forward with requests, until the server has no more requests in its queue
    # This lets us get full utilization by having many workers, but also to be outputting dolma docs as soon as possible
    # As soon as one worker is no longer saturating the gpu, the next one can start sending requests
    # semaphore = asyncio.Semaphore(1)

    # await sglang_server_task(semaphore)

    # await sglang_server_ready()
    print("OLM OCR model loaded ✅")
    
    yield

    # Wait for server to stop
    # process_pool.shutdown(wait=False)

    # sglang_server.cancel()
    # logger.info("Work done")
    
    del model
    del processor
    torch.cuda.empty_cache()
    gc.collect()

olm_ocr = APIRouter(lifespan=lifespan)

# async def process_page(args, worker_id: int, pdf_orig_path: str, pdf_local_path: str, page_num: int) -> PageResult:
#     COMPLETION_URL = f"http://localhost:{SGLANG_SERVER_PORT}/v1/chat/completions"
#     MAX_RETRIES = 5

#     exponential_backoffs = 0
#     local_anchor_text_len = 6000
#     local_image_rotation = 0
#     attempt = 0
#     await tracker.track_work(worker_id, f"{pdf_orig_path}-{page_num}", "started")

#     while attempt < MAX_RETRIES:
#         query = await build_page_query(pdf_local_path, page_num, 1024, local_anchor_text_len, image_rotation=local_image_rotation)

#         logger.info(f"Built page query for {pdf_orig_path}-{page_num}")

#         try:
#             status_code, response_body = await apost(COMPLETION_URL, json_data=query)

#             if status_code == 400:
#                 raise ValueError(f"Got BadRequestError from server: {response_body}, skipping this response")
#             elif status_code == 500:
#                 raise ValueError(f"Got InternalServerError from server: {response_body}, skipping this response")
#             elif status_code != 200:
#                 raise ValueError(f"Error http status {status_code}")

#             base_response_data = json.loads(response_body)

#             if base_response_data["usage"]["total_tokens"] > args.model_max_context:
#                 local_anchor_text_len = max(1, local_anchor_text_len // 2)
#                 logger.info(f"Reducing anchor text len to {local_anchor_text_len} for {pdf_orig_path}-{page_num}")
#                 raise ValueError("Response exceeded model_max_context, cannot use this response")

#             metrics.add_metrics(
#                 sglang_input_tokens=base_response_data["usage"].get("prompt_tokens", 0),
#                 sglang_output_tokens=base_response_data["usage"].get("completion_tokens", 0),
#             )

#             model_response_json = json.loads(base_response_data["choices"][0]["message"]["content"])
#             page_response = PageResponse(**model_response_json)

#             if not page_response.is_rotation_valid and attempt < MAX_RETRIES - 1:
#                 logger.info(
#                     f"Got invalid_page rotation for {pdf_orig_path}-{page_num} attempt {attempt}, retrying with {page_response.rotation_correction} rotation"
#                 )
#                 local_image_rotation = page_response.rotation_correction
#                 raise ValueError(f"invalid_page rotation for {pdf_orig_path}-{page_num}")

#             await tracker.track_work(worker_id, f"{pdf_orig_path}-{page_num}", "finished")
#             return PageResult(
#                 pdf_orig_path,
#                 page_num,
#                 page_response,
#                 input_tokens=base_response_data["usage"].get("prompt_tokens", 0),
#                 output_tokens=base_response_data["usage"].get("completion_tokens", 0),
#                 is_fallback=False,
#             )
#         except (ConnectionError, OSError, asyncio.TimeoutError) as e:
#             logger.warning(f"Client error on attempt {attempt} for {pdf_orig_path}-{page_num}: {type(e)} {e}")

#             # Now we want to do exponential backoff, and not count this as an actual page retry
#             # Page retrys are supposed to be for fixing bad results from the model, but actual requests to sglang
#             # are supposed to work. Probably this means that the server is just restarting
#             sleep_delay = 10 * (2**exponential_backoffs)
#             exponential_backoffs += 1
#             logger.info(f"Sleeping for {sleep_delay} seconds on {pdf_orig_path}-{page_num} to allow server restart")
#             await asyncio.sleep(sleep_delay)
#         except asyncio.CancelledError:
#             logger.info(f"Process page {pdf_orig_path}-{page_num} cancelled")
#             await tracker.track_work(worker_id, f"{pdf_orig_path}-{page_num}", "cancelled")
#             raise
#         except json.JSONDecodeError as e:
#             logger.warning(f"JSON decode error on attempt {attempt} for {pdf_orig_path}-{page_num}: {e}")
#             attempt += 1
#         except ValueError as e:
#             logger.warning(f"ValueError on attempt {attempt} for {pdf_orig_path}-{page_num}: {type(e)} - {e}")
#             attempt += 1
#         except Exception as e:
#             logger.exception(f"Unexpected error on attempt {attempt} for {pdf_orig_path}-{page_num}: {type(e)} - {e}")
#             attempt += 1

#     logger.error(f"Failed to process {pdf_orig_path}-{page_num} after {MAX_RETRIES} attempts.")
#     await tracker.track_work(worker_id, f"{pdf_orig_path}-{page_num}", "errored")

#     return PageResult(
#         pdf_orig_path,
#         page_num,
#         PageResponse(
#             natural_text=get_anchor_text(pdf_local_path, page_num, pdf_engine="pdftotext"),
#             primary_language=None,
#             is_rotation_valid=True,
#             rotation_correction=0,
#             is_table=False,
#             is_diagram=False,
#         ),
#         input_tokens=0,
#         output_tokens=0,
#         is_fallback=True,
#     )


# async def process_pdf(args, worker_id: int, pdf_orig_path: str):
#     with tempfile.NamedTemporaryFile("wb+", suffix=".pdf") as tf:
#         try:
#             data = await asyncio.to_thread(lambda: get_s3_bytes_with_backoff(pdf_s3, pdf_orig_path))
#             tf.write(data)
#             tf.flush()
#         except ClientError as ex:
#             if ex.response["Error"]["Code"] == "NoSuchKey":
#                 logger.info(f"S3 File Not found, skipping it completely {pdf_orig_path}")
#                 return None
#             else:
#                 raise

#         try:
#             reader = PdfReader(tf.name)
#             num_pages = reader.get_num_pages()
#         except:
#             logger.exception(f"Could not count number of pages for {pdf_orig_path}, aborting document")
#             return None

#         logger.info(f"Got {num_pages} pages to do for {pdf_orig_path} in worker {worker_id}")

#         if args.apply_filter and get_pdf_filter().filter_out_pdf(tf.name):
#             logger.info(f"Filtering out pdf {pdf_orig_path}")
#             return None

#         # List to hold the tasks for processing each page
#         page_tasks = []
#         page_results = []

#         try:
#             async with asyncio.TaskGroup() as tg:
#                 for page_num in range(1, num_pages + 1):
#                     task = tg.create_task(process_page(args, worker_id, pdf_orig_path, tf.name, page_num))
#                     page_tasks.append(task)

#             # Collect the results from the entire task group, assuming no exceptions
#             page_results = [task.result() for task in page_tasks]

#             num_fallback_pages = sum(page_result.is_fallback for page_result in page_results)

#             if num_fallback_pages / num_pages > args.max_page_error_rate:
#                 logger.error(
#                     f"Document {pdf_orig_path} has {num_fallback_pages} fallback pages out of {num_pages} exceeding max_page_error_rate of {args.max_page_error_rate}, discarding document."
#                 )
#                 return None
#             elif num_fallback_pages > 0:
#                 logger.warning(
#                     f"Document {pdf_orig_path} processed with {num_fallback_pages} fallback pages out of {num_pages}, proceeding to build Dolma document."
#                 )

#             return build_dolma_document(pdf_orig_path, page_results)
#         except Exception as e:
#             # Check for ExceptionGroup with BrokenProcessPool
#             if isinstance(e, ExceptionGroup):
#                 broken_pool, other = e.split(BrokenProcessPool)
#                 if broken_pool is not None:  # Found at least one BrokenProcessPool
#                     logger.critical("Encountered BrokenProcessPool, exiting process.")
#                     sys.exit(1)

#             logger.exception(f"Exception in process_pdf for {pdf_orig_path}: {e}")
#             # You can't build a dolma doc with even 1 failed page, so just get out of here
#             # However, you don't want to propagate an exception higher up and cancel the entire work_group
#             return None


def process_page(page_num, barcode, temperature, dpi, max_new_tokens, num_return_sequences, device):
    """Process a single page and return the OCR text with performance metrics."""
    perf_metrics = {}
    total_start = time.perf_counter()
    
    # Render page to an image
    render_start = time.perf_counter()
    # image_base64 = render_pdf_to_base64png(pdf_path, page_num, target_longest_image_dim=2048)
    try:
        # Download image from Supabase Storage
        response = (
            supabase.storage
            .from_("ia_bucket")
            .download(f"{barcode}/{page_num}.png")
        )

        print(f"Downloaded image for barcode {barcode}, page {page_num}")

        # Convert response bytes to base64
        image_base64 = base64.b64encode(response).decode('utf-8')
    except Exception as e:
        raise HTTPException(status_code=404, detail=f"Could not find image for barcode {barcode}, page {page_num}: {str(e)}")
    render_end = time.perf_counter()
    perf_metrics["render_time"] = render_end - render_start
    
    # Get anchor text
    anchor_start = time.perf_counter()
    # try:
    #     anchor_text = get_anchor_text(pdf_path, page_num, pdf_engine="pdfreport", target_length=4000)
    #     prompt = build_finetuning_prompt(anchor_text)
    # except:
    #     prompt = ""
    prompt = "Following is a scanned government document page. Return the text content of the page."
    anchor_end = time.perf_counter()
    perf_metrics["anchor_text_time"] = anchor_end - anchor_start
    
    # Build the full prompt
    prep_start = time.perf_counter()
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_base64}"}},
            ],
        }
    ]
    
    # Apply the chat template and processor
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    main_image = Image.open(BytesIO(base64.b64decode(image_base64)))
    inputs = processor(
        text=[text],
        images=[main_image],
        padding=True,
        return_tensors="pt",
    )
    inputs = {key: value.to(device) for (key, value) in inputs.items()}
    prep_end = time.perf_counter()
    perf_metrics["preprocessing_time"] = prep_end - prep_start
    
    # Generate the output
    inference_start = time.perf_counter()
    output = model.generate(
        **inputs,
        temperature=temperature,
        max_new_tokens=max_new_tokens,
        num_return_sequences=num_return_sequences,
        do_sample=True,
    )
    inference_end = time.perf_counter()
    perf_metrics["inference_time"] = inference_end - inference_start
    
    # Decode the output
    postproc_start = time.perf_counter()
    prompt_length = inputs["input_ids"].shape[1]
    new_tokens = output[:, prompt_length:]
    text_output = processor.tokenizer.batch_decode(
        new_tokens, skip_special_tokens=True
    )
    
    try:
        page_text = json.loads(text_output[0])['natural_text']
    except:
        page_text = text_output[0]
    postproc_end = time.perf_counter()
    perf_metrics["postprocessing_time"] = postproc_end - postproc_start
    
    total_end = time.perf_counter()
    perf_metrics["total_time"] = total_end - total_start
    
    return {
        "page_number": page_num, 
        "text": page_text,
        "performance": perf_metrics
    }

@olm_ocr.get("/olmocr")
def olm(barcode: str, last_page: int, first_page: int = 1, temprature: float = 0.9, dpi:int = 256, max_new_tokens: int = 5000, num_return_sequences: int = 1):
    """
    Perform OCR on a specific page of the given PDF using Tesseract.
    
    Args:
        barcode: Barcode identifier for the document
        first_page: First Page number to OCR (1-based index)
        last_page: Last Page number to OCR (1-based index)
        temprature: The value used to control the randomness of the generated text
        max_new_tokens: The maximum number of tokens to generate
        num_return_sequences: The number of sequences to generate
    
    Returns:
        JSON response with OCR'd text for the specified page(s)
    """
    api_start = time.perf_counter()
    
    # # Locate the PDF file
    # pdf_locate_start = time.perf_counter()
    # try:
    #     script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    #     pdf_path = os.path.join(script_dir, "pdfs", pdf_path)
    # except Exception as e:
    #     raise HTTPException(status_code=500, detail="Error rendering PDF: " + str(e))
    # pdf_locate_end = time.perf_counter()
    
    # Get the number of pages in the PDF
    # page_count_start = time.perf_counter()
    # num_pages = total_pages(pdf_path)
    # page_count_end = time.perf_counter()

    #print(f"Total number of pages of {pdf_path}: {num_pages}")
    
    # Configure processing parameters
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    output = []

    # Process pages
    processing_start = time.perf_counter()
    for page in range(first_page, last_page + 1):  # num_pages
        page_start = time.perf_counter()
        output.append(process_page(
            page_num=page,
            barcode=barcode, 
            temperature=temprature, 
            dpi=dpi, 
            max_new_tokens=max_new_tokens, 
            num_return_sequences=num_return_sequences,
            device=device
        ))
        page_end = time.perf_counter()
        print(f"Page {page+1} processed in {page_end - page_start:.2f}s")
    
    processing_end = time.perf_counter()
    
    # Sort and prepare results
    output.sort(key=lambda x: x['page_number'])
    
    # Calculate overall performance metrics
    api_end = time.perf_counter()
    
    # Add overall performance metrics
    performance_summary = {
        # "pdf_location_time": pdf_locate_end - pdf_locate_start,
        # "page_counting_time": page_count_end - page_count_start,
        "processing_time": processing_end - processing_start,
        "total_api_time": api_end - api_start,
        "pages_processed": len(output),
        "avg_page_processing_time": (processing_end - processing_start) / max(1, len(output))
    }
    
    # Return with performance metrics
    return {
        "pages": output,
        "performance_summary": performance_summary
    }


    # # Use ThreadPoolExecutor since we're primarily I/O bound with the model inference
    # with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
    #     # Create a partial function with the fixed parameters
    #     process_func = partial(
    #         process_page, 
    #         pdf_path=pdf_path, 
    #         temperature=temprature, 
    #         dpi=dpi, 
    #         max_new_tokens=max_new_tokens, 
    #         num_return_sequences=num_return_sequences,
    #         device=device
    #     )
        
    #     # Process all pages in parallel
    #     for result in executor.map(process_func, range(3)): #num_pages for entire pdf
    #         result_dict["pages"].append(result)
    #         full_text += f"\n\n--- PAGE {result['page']} ---\n\n{result['text']}"

    # #Sort the pages by page number
    # result_dict["pages"] = sorted(result_dict["pages"], key=lambda x: x["page"])
    # result_dict["full_text"] = full_text
    
    # return result_dict