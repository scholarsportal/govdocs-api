2025-03-21 10:34:39,512 - olm-ocr-api - INFO - /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
2025-03-21 10:34:39,513 - olm-ocr-api - INFO -   warnings.warn(
2025-03-21 10:34:41,934 - olm-ocr-api - INFO - INFO 03-21 10:34:41 __init__.py:190] Automatically detected platform cuda.
2025-03-21 10:34:44,848 - olm-ocr-api - INFO - [2025-03-21 10:34:44] server_args=ServerArgs(model_path='allenai/olmOCR-7B-0225-preview', tokenizer_path='allenai/olmOCR-7B-0225-preview', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='allenai/olmOCR-7B-0225-preview', chat_template='qwen2-vl', is_embedding=False, revision=None, host='127.0.0.1', port=30024, mem_fraction_static=0.7, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, stream_interval=1, stream_output=False, random_seed=535075269, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http='warning', log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=5, speculative_eagle_topk=4, speculative_num_draft_tokens=8, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, enable_flashinfer_mla=False, flashinfer_mla_disable_ragged=False, warmups=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False)
2025-03-21 10:34:47,119 - olm-ocr-api - INFO - [2025-03-21 10:34:47] Use chat template for the OpenAI-compatible API server: qwen2-vl
2025-03-21 10:34:48,078 - olm-ocr-api - INFO - /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
2025-03-21 10:34:48,078 - olm-ocr-api - INFO -   warnings.warn(
2025-03-21 10:34:48,106 - olm-ocr-api - INFO - /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
2025-03-21 10:34:48,106 - olm-ocr-api - INFO -   warnings.warn(
2025-03-21 10:34:50,771 - olm-ocr-api - INFO - INFO 03-21 10:34:50 __init__.py:190] Automatically detected platform cuda.
2025-03-21 10:34:50,775 - olm-ocr-api - INFO - INFO 03-21 10:34:50 __init__.py:190] Automatically detected platform cuda.
2025-03-21 10:34:54,132 - olm-ocr-api - INFO - [2025-03-21 10:34:54 TP0] Overlap scheduler is disabled for multimodal models.
2025-03-21 10:34:54,170 - olm-ocr-api - INFO - [2025-03-21 10:34:54 TP0] Automatically reduce --mem-fraction-static to 0.665 because this is a multimodal model.
2025-03-21 10:34:54,170 - olm-ocr-api - INFO - [2025-03-21 10:34:54 TP0] Automatically turn off --chunked-prefill-size and disable radix cache for qwen2-vl.
2025-03-21 10:34:54,170 - olm-ocr-api - INFO - [2025-03-21 10:34:54 TP0] Init torch distributed begin.
2025-03-21 10:34:54,321 - olm-ocr-api - INFO - [2025-03-21 10:34:54 TP0] Init torch distributed ends. mem usage=0.00 GB
2025-03-21 10:34:54,321 - olm-ocr-api - INFO - [2025-03-21 10:34:54 TP0] Load weight begin. avail mem=30.84 GB
2025-03-21 10:34:54,414 - olm-ocr-api - INFO - [2025-03-21 10:34:54 TP0] The following error message 'operation scheduled before its operands' can be ignored.
2025-03-21 10:34:54,679 - olm-ocr-api - INFO - [2025-03-21 10:34:54 TP0] Using model weights format ['*.safetensors']
2025-03-21 10:34:54,758 - olm-ocr-api - INFO - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
2025-03-21 10:34:59,684 - olm-ocr-api - INFO - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:14,  4.93s/it]
2025-03-21 10:35:04,500 - olm-ocr-api - INFO - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.86s/it]
2025-03-21 10:35:05,801 - olm-ocr-api - INFO - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:11<00:03,  3.24s/it]
2025-03-21 10:35:06,554 - olm-ocr-api - INFO - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.26s/it]
2025-03-21 10:35:06,554 - olm-ocr-api - INFO - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.95s/it]
2025-03-21 10:35:06,554 - olm-ocr-api - INFO - 
2025-03-21 10:35:06,944 - olm-ocr-api - INFO - [2025-03-21 10:35:06 TP0] Load weight end. type=Qwen2VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=15.12 GB, mem usage=15.72 GB.
2025-03-21 10:35:07,451 - olm-ocr-api - INFO - [2025-03-21 10:35:07 TP0] KV Cache is allocated. #tokens: 89666, K size: 2.39 GB, V size: 2.39 GB
2025-03-21 10:35:07,451 - olm-ocr-api - INFO - [2025-03-21 10:35:07 TP0] Memory pool end. avail mem=10.04 GB
2025-03-21 10:35:07,626 - olm-ocr-api - INFO - 2025-03-21 10:35:07,626 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend
2025-03-21 10:35:07,647 - olm-ocr-api - INFO - [2025-03-21 10:35:07 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=9.54 GB
2025-03-21 10:35:08,514 - olm-ocr-api - INFO -   0%|          | 0/23 [00:00<?, ?it/s]Capturing batches (avail_mem=9.50 GB):   0%|          | 0/23 [00:00<?, ?it/s]2025-03-21 10:35:08,514 - INFO - flashinfer.jit: Loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False
2025-03-21 10:35:09,976 - olm-ocr-api - INFO - Capturing batches (avail_mem=9.50 GB):   0%|          | 0/23 [00:02<?, ?it/s]
2025-03-21 10:35:09,979 - olm-ocr-api - INFO - [2025-03-21 10:35:09 TP0] Scheduler hit an exception: Traceback (most recent call last):
2025-03-21 10:35:09,979 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2104, in _run_ninja_build
2025-03-21 10:35:09,979 - olm-ocr-api - INFO -     subprocess.run(
2025-03-21 10:35:09,979 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/subprocess.py", line 571, in run
2025-03-21 10:35:09,979 - olm-ocr-api - INFO -     raise CalledProcessError(retcode, process.args,
2025-03-21 10:35:09,979 - olm-ocr-api - INFO - subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.
2025-03-21 10:35:09,979 - olm-ocr-api - INFO - 
2025-03-21 10:35:09,979 - olm-ocr-api - INFO - The above exception was the direct cause of the following exception:
2025-03-21 10:35:09,979 - olm-ocr-api - INFO - 
2025-03-21 10:35:09,979 - olm-ocr-api - INFO - Traceback (most recent call last):
2025-03-21 10:35:09,980 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 252, in __init__
2025-03-21 10:35:09,980 - olm-ocr-api - INFO -     self.capture()
2025-03-21 10:35:09,980 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 336, in capture
2025-03-21 10:35:09,980 - olm-ocr-api - INFO -     ) = self.capture_one_batch_size(bs, forward)
2025-03-21 10:35:09,980 - olm-ocr-api - INFO -         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-03-21 10:35:09,980 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 406, in capture_one_batch_size
2025-03-21 10:35:09,980 - olm-ocr-api - INFO -     self.model_runner.attn_backend.init_forward_metadata_capture_cuda_graph(
2025-03-21 10:35:09,980 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/layers/attention/flashinfer_backend.py", line 303, in init_forward_metadata_capture_cuda_graph
2025-03-21 10:35:09,980 - olm-ocr-api - INFO -     self.indices_updater_decode.update(
2025-03-21 10:35:09,980 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/layers/attention/flashinfer_backend.py", line 553, in update_single_wrapper
2025-03-21 10:35:09,980 - olm-ocr-api - INFO -     self.call_begin_forward(
2025-03-21 10:35:09,980 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/layers/attention/flashinfer_backend.py", line 663, in call_begin_forward
2025-03-21 10:35:09,980 - olm-ocr-api - INFO -     wrapper.begin_forward(
2025-03-21 10:35:09,980 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/decode.py", line 867, in plan
2025-03-21 10:35:09,980 - olm-ocr-api - INFO -     self._cached_module = get_batch_prefill_module("fa2")(
2025-03-21 10:35:09,980 - olm-ocr-api - INFO -                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-03-21 10:35:09,980 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/prefill.py", line 197, in backend_module
2025-03-21 10:35:09,981 - olm-ocr-api - INFO -     module = gen_batch_prefill_module(backend, *args)
2025-03-21 10:35:09,981 - olm-ocr-api - INFO -              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-03-21 10:35:09,981 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/jit/attention/pytorch.py", line 568, in gen_batch_prefill_module
2025-03-21 10:35:09,981 - olm-ocr-api - INFO -     return gen_customize_batch_prefill_module(
2025-03-21 10:35:09,981 - olm-ocr-api - INFO -            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-03-21 10:35:09,981 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/jit/attention/pytorch.py", line 1012, in gen_customize_batch_prefill_module
2025-03-21 10:35:09,981 - olm-ocr-api - INFO -     return load_cuda_ops(
2025-03-21 10:35:09,981 - olm-ocr-api - INFO -            ^^^^^^^^^^^^^^
2025-03-21 10:35:09,981 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/jit/core.py", line 123, in load_cuda_ops
2025-03-21 10:35:09,981 - olm-ocr-api - INFO -     torch_cpp_ext.load(
2025-03-21 10:35:09,981 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1314, in load
2025-03-21 10:35:09,981 - olm-ocr-api - INFO -     return _jit_compile(
2025-03-21 10:35:09,981 - olm-ocr-api - INFO -            ^^^^^^^^^^^^^
2025-03-21 10:35:09,981 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1721, in _jit_compile
2025-03-21 10:35:09,981 - olm-ocr-api - INFO -     _write_ninja_file_and_build_library(
2025-03-21 10:35:09,981 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1833, in _write_ninja_file_and_build_library
2025-03-21 10:35:09,982 - olm-ocr-api - INFO -     _run_ninja_build(
2025-03-21 10:35:09,982 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2120, in _run_ninja_build
2025-03-21 10:35:09,982 - olm-ocr-api - INFO -     raise RuntimeError(message) from e
2025-03-21 10:35:09,982 - olm-ocr-api - INFO - RuntimeError: Error building extension 'batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False': [1/3] /local/home/hfurquan/myProjects/CUDA/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_prefill_jit_pybind.cuda.o.d -DTORCH_EXTENSION_NAME=batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/TH -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/THC -isystem /local/home/hfurquan/myProjects/CUDA/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 --expt-relaxed-constexpr -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -std=c++17 --threads 4 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -c /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu -o batch_prefill_jit_pybind.cuda.o
2025-03-21 10:35:09,982 - olm-ocr-api - INFO - FAILED: batch_prefill_jit_pybind.cuda.o
2025-03-21 10:35:09,982 - olm-ocr-api - INFO - /local/home/hfurquan/myProjects/CUDA/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_prefill_jit_pybind.cuda.o.d -DTORCH_EXTENSION_NAME=batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/TH -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/THC -isystem /local/home/hfurquan/myProjects/CUDA/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 --expt-relaxed-constexpr -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -std=c++17 --threads 4 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -c /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu -o batch_prefill_jit_pybind.cuda.o
2025-03-21 10:35:09,982 - olm-ocr-api - INFO - In file included from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBase.h:14,
2025-03-21 10:35:09,982 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBody.h:38,
2025-03-21 10:35:09,982 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/jit_type.h:5,
2025-03-21 10:35:09,982 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/function_schema.h:6,
2025-03-21 10:35:09,982 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/op_registration/infer_schema.h:8,
2025-03-21 10:35:09,982 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/library.h:61,
2025-03-21 10:35:09,982 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc/pytorch_extension_utils.h:19,
2025-03-21 10:35:09,983 - olm-ocr-api - INFO -                  from /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu:17:
2025-03-21 10:35:09,983 - olm-ocr-api - INFO - /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/c10/util/C++17.h:13:2: error: #error "You're trying to build PyTorch with a too old version of GCC. We need GCC 9 or later."
2025-03-21 10:35:09,983 - olm-ocr-api - INFO -  #error \
2025-03-21 10:35:09,983 - olm-ocr-api - INFO -   ^~~~~
2025-03-21 10:35:09,983 - olm-ocr-api - INFO - In file included from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBase.h:14,
2025-03-21 10:35:09,983 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBody.h:38,
2025-03-21 10:35:09,983 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/jit_type.h:5,
2025-03-21 10:35:09,983 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/function_schema.h:6,
2025-03-21 10:35:09,983 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/op_registration/infer_schema.h:8,
2025-03-21 10:35:09,983 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/library.h:61,
2025-03-21 10:35:09,983 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc/pytorch_extension_utils.h:19,
2025-03-21 10:35:09,983 - olm-ocr-api - INFO -                  from /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu:17:
2025-03-21 10:35:09,983 - olm-ocr-api - INFO - /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/c10/util/C++17.h:13:2: error: #error "You're trying to build PyTorch with a too old version of GCC. We need GCC 9 or later."
2025-03-21 10:35:09,983 - olm-ocr-api - INFO -  #error \
2025-03-21 10:35:09,983 - olm-ocr-api - INFO -   ^~~~~
2025-03-21 10:35:09,983 - olm-ocr-api - INFO - fatal   : Could not open input file /tmp/tmpxft_0034c1e4_00000000-7_batch_prefill_jit_pybind.cpp1.ii
2025-03-21 10:35:09,984 - olm-ocr-api - INFO - [2/3] /local/home/hfurquan/myProjects/CUDA/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_prefill.cuda.o.d -DTORCH_EXTENSION_NAME=batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/TH -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/THC -isystem /local/home/hfurquan/myProjects/CUDA/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 --expt-relaxed-constexpr -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -std=c++17 --threads 4 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -c /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cu -o batch_prefill.cuda.o
2025-03-21 10:35:09,984 - olm-ocr-api - INFO - FAILED: batch_prefill.cuda.o
2025-03-21 10:35:09,984 - olm-ocr-api - INFO - /local/home/hfurquan/myProjects/CUDA/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_prefill.cuda.o.d -DTORCH_EXTENSION_NAME=batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/TH -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/THC -isystem /local/home/hfurquan/myProjects/CUDA/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 --expt-relaxed-constexpr -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -std=c++17 --threads 4 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -c /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cu -o batch_prefill.cuda.o
2025-03-21 10:35:09,984 - olm-ocr-api - INFO - In file included from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBase.h:14,
2025-03-21 10:35:09,984 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBody.h:38,
2025-03-21 10:35:09,984 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/jit_type.h:5,
2025-03-21 10:35:09,984 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/function_schema.h:6,
2025-03-21 10:35:09,984 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/op_registration/infer_schema.h:8,
2025-03-21 10:35:09,984 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/library.h:61,
2025-03-21 10:35:09,984 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc/pytorch_extension_utils.h:19,
2025-03-21 10:35:09,984 - olm-ocr-api - INFO -                  from /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cu:22:
2025-03-21 10:35:09,984 - olm-ocr-api - INFO - /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/c10/util/C++17.h:13:2: error: #error "You're trying to build PyTorch with a too old version of GCC. We need GCC 9 or later."
2025-03-21 10:35:09,984 - olm-ocr-api - INFO -  #error \
2025-03-21 10:35:09,984 - olm-ocr-api - INFO -   ^~~~~
2025-03-21 10:35:09,984 - olm-ocr-api - INFO - In file included from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBase.h:14,
2025-03-21 10:35:09,985 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBody.h:38,
2025-03-21 10:35:09,985 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/jit_type.h:5,
2025-03-21 10:35:09,985 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/function_schema.h:6,
2025-03-21 10:35:09,985 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/op_registration/infer_schema.h:8,
2025-03-21 10:35:09,985 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/library.h:61,
2025-03-21 10:35:09,985 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc/pytorch_extension_utils.h:19,
2025-03-21 10:35:09,985 - olm-ocr-api - INFO -                  from /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cu:22:
2025-03-21 10:35:09,985 - olm-ocr-api - INFO - /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/c10/util/C++17.h:13:2: error: #error "You're trying to build PyTorch with a too old version of GCC. We need GCC 9 or later."
2025-03-21 10:35:09,985 - olm-ocr-api - INFO -  #error \
2025-03-21 10:35:09,985 - olm-ocr-api - INFO -   ^~~~~
2025-03-21 10:35:09,985 - olm-ocr-api - INFO - fatal   : Could not open input file /tmp/tmpxft_0034c1e3_00000000-7_batch_prefill.cpp1.ii
2025-03-21 10:35:09,985 - olm-ocr-api - INFO - ninja: build stopped: subcommand failed.
2025-03-21 10:35:09,985 - olm-ocr-api - INFO - 
2025-03-21 10:35:09,985 - olm-ocr-api - INFO - 
2025-03-21 10:35:09,985 - olm-ocr-api - INFO - During handling of the above exception, another exception occurred:
2025-03-21 10:35:09,986 - olm-ocr-api - INFO - 
2025-03-21 10:35:09,986 - olm-ocr-api - INFO - Traceback (most recent call last):
2025-03-21 10:35:09,986 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 1748, in run_scheduler_process
2025-03-21 10:35:09,986 - olm-ocr-api - INFO -     scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)
2025-03-21 10:35:09,986 - olm-ocr-api - INFO -                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-03-21 10:35:09,986 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 218, in __init__
2025-03-21 10:35:09,986 - olm-ocr-api - INFO -     self.tp_worker = TpWorkerClass(
2025-03-21 10:35:09,986 - olm-ocr-api - INFO -                      ^^^^^^^^^^^^^^
2025-03-21 10:35:09,986 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py", line 74, in __init__
2025-03-21 10:35:09,986 - olm-ocr-api - INFO -     self.model_runner = ModelRunner(
2025-03-21 10:35:09,986 - olm-ocr-api - INFO -                         ^^^^^^^^^^^^
2025-03-21 10:35:09,986 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py", line 166, in __init__
2025-03-21 10:35:09,986 - olm-ocr-api - INFO -     self.initialize(min_per_gpu_memory)
2025-03-21 10:35:09,986 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py", line 207, in initialize
2025-03-21 10:35:09,986 - olm-ocr-api - INFO -     self.init_cuda_graphs()
2025-03-21 10:35:09,986 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py", line 881, in init_cuda_graphs
2025-03-21 10:35:09,986 - olm-ocr-api - INFO -     self.cuda_graph_runner = CudaGraphRunner(self)
2025-03-21 10:35:09,987 - olm-ocr-api - INFO -                              ^^^^^^^^^^^^^^^^^^^^^
2025-03-21 10:35:09,987 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 254, in __init__
2025-03-21 10:35:09,987 - olm-ocr-api - INFO -     raise Exception(
2025-03-21 10:35:09,987 - olm-ocr-api - INFO - Exception: Capture cuda graph failed: Error building extension 'batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False': [1/3] /local/home/hfurquan/myProjects/CUDA/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_prefill_jit_pybind.cuda.o.d -DTORCH_EXTENSION_NAME=batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/TH -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/THC -isystem /local/home/hfurquan/myProjects/CUDA/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 --expt-relaxed-constexpr -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -std=c++17 --threads 4 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -c /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu -o batch_prefill_jit_pybind.cuda.o
2025-03-21 10:35:09,987 - olm-ocr-api - INFO - FAILED: batch_prefill_jit_pybind.cuda.o
2025-03-21 10:35:09,987 - olm-ocr-api - INFO - /local/home/hfurquan/myProjects/CUDA/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_prefill_jit_pybind.cuda.o.d -DTORCH_EXTENSION_NAME=batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/TH -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/THC -isystem /local/home/hfurquan/myProjects/CUDA/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 --expt-relaxed-constexpr -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -std=c++17 --threads 4 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -c /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu -o batch_prefill_jit_pybind.cuda.o
2025-03-21 10:35:09,988 - olm-ocr-api - INFO - In file included from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBase.h:14,
2025-03-21 10:35:09,988 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBody.h:38,
2025-03-21 10:35:09,988 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/jit_type.h:5,
2025-03-21 10:35:09,988 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/function_schema.h:6,
2025-03-21 10:35:09,988 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/op_registration/infer_schema.h:8,
2025-03-21 10:35:09,988 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/library.h:61,
2025-03-21 10:35:09,988 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc/pytorch_extension_utils.h:19,
2025-03-21 10:35:09,988 - olm-ocr-api - INFO -                  from /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu:17:
2025-03-21 10:35:09,988 - olm-ocr-api - INFO - /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/c10/util/C++17.h:13:2: error: #error "You're trying to build PyTorch with a too old version of GCC. We need GCC 9 or later."
2025-03-21 10:35:09,989 - olm-ocr-api - INFO -  #error \
2025-03-21 10:35:09,989 - olm-ocr-api - INFO -   ^~~~~
2025-03-21 10:35:09,989 - olm-ocr-api - INFO - In file included from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBase.h:14,
2025-03-21 10:35:09,989 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBody.h:38,
2025-03-21 10:35:09,989 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/jit_type.h:5,
2025-03-21 10:35:09,989 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/function_schema.h:6,
2025-03-21 10:35:09,989 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/op_registration/infer_schema.h:8,
2025-03-21 10:35:09,989 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/library.h:61,
2025-03-21 10:35:09,989 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc/pytorch_extension_utils.h:19,
2025-03-21 10:35:09,989 - olm-ocr-api - INFO -                  from /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu:17:
2025-03-21 10:35:09,989 - olm-ocr-api - INFO - /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/c10/util/C++17.h:13:2: error: #error "You're trying to build PyTorch with a too old version of GCC. We need GCC 9 or later."
2025-03-21 10:35:09,989 - olm-ocr-api - INFO -  #error \
2025-03-21 10:35:09,989 - olm-ocr-api - INFO -   ^~~~~
2025-03-21 10:35:09,989 - olm-ocr-api - INFO - fatal   : Could not open input file /tmp/tmpxft_0034c1e4_00000000-7_batch_prefill_jit_pybind.cpp1.ii
2025-03-21 10:35:09,989 - olm-ocr-api - INFO - [2/3] /local/home/hfurquan/myProjects/CUDA/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_prefill.cuda.o.d -DTORCH_EXTENSION_NAME=batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/TH -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/THC -isystem /local/home/hfurquan/myProjects/CUDA/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 --expt-relaxed-constexpr -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -std=c++17 --threads 4 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -c /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cu -o batch_prefill.cuda.o
2025-03-21 10:35:09,990 - olm-ocr-api - INFO - FAILED: batch_prefill.cuda.o
2025-03-21 10:35:09,990 - olm-ocr-api - INFO - /local/home/hfurquan/myProjects/CUDA/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_prefill.cuda.o.d -DTORCH_EXTENSION_NAME=batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/TH -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/THC -isystem /local/home/hfurquan/myProjects/CUDA/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 --expt-relaxed-constexpr -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -std=c++17 --threads 4 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -c /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cu -o batch_prefill.cuda.o
2025-03-21 10:35:09,990 - olm-ocr-api - INFO - In file included from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBase.h:14,
2025-03-21 10:35:09,990 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBody.h:38,
2025-03-21 10:35:09,990 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/jit_type.h:5,
2025-03-21 10:35:09,990 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/function_schema.h:6,
2025-03-21 10:35:09,990 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/op_registration/infer_schema.h:8,
2025-03-21 10:35:09,990 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/library.h:61,
2025-03-21 10:35:09,990 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc/pytorch_extension_utils.h:19,
2025-03-21 10:35:09,990 - olm-ocr-api - INFO -                  from /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cu:22:
2025-03-21 10:35:09,990 - olm-ocr-api - INFO - /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/c10/util/C++17.h:13:2: error: #error "You're trying to build PyTorch with a too old version of GCC. We need GCC 9 or later."
2025-03-21 10:35:09,990 - olm-ocr-api - INFO -  #error \
2025-03-21 10:35:09,990 - olm-ocr-api - INFO -   ^~~~~
2025-03-21 10:35:09,990 - olm-ocr-api - INFO - In file included from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBase.h:14,
2025-03-21 10:35:09,990 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBody.h:38,
2025-03-21 10:35:09,991 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/jit_type.h:5,
2025-03-21 10:35:09,991 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/function_schema.h:6,
2025-03-21 10:35:09,991 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/op_registration/infer_schema.h:8,
2025-03-21 10:35:09,991 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/library.h:61,
2025-03-21 10:35:09,991 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc/pytorch_extension_utils.h:19,
2025-03-21 10:35:10,151 - olm-ocr-api - INFO -                  from /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cu:22:
2025-03-21 10:35:10,151 - olm-ocr-api - INFO - /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/c10/util/C++17.h:13:2: error: #error "You're trying to build PyTorch with a too old version of GCC. We need GCC 9 or later."
2025-03-21 10:35:10,151 - olm-ocr-api - INFO -  #error \
2025-03-21 10:35:10,151 - olm-ocr-api - INFO -   ^~~~~
2025-03-21 10:35:10,151 - olm-ocr-api - INFO - fatal   : Could not open input file /tmp/tmpxft_0034c1e3_00000000-7_batch_prefill.cpp1.ii
2025-03-21 10:35:10,151 - olm-ocr-api - INFO - ninja: build stopped: subcommand failed.
2025-03-21 10:35:10,152 - olm-ocr-api - INFO - 
2025-03-21 10:35:10,152 - olm-ocr-api - INFO - Possible solutions:
2025-03-21 10:35:10,152 - olm-ocr-api - INFO - 1. disable cuda graph by --disable-cuda-graph
2025-03-21 10:35:10,152 - olm-ocr-api - INFO - 2. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
2025-03-21 10:35:10,152 - olm-ocr-api - INFO - 3. disable torch compile by not using --enable-torch-compile
2025-03-21 10:35:10,152 - olm-ocr-api - INFO - 4. set --cuda-graph-max-bs to a smaller value (e.g., 32)
2025-03-21 10:35:10,152 - olm-ocr-api - INFO - Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose
2025-03-21 10:35:10,152 - olm-ocr-api - INFO - 
2025-03-21 10:35:10,152 - olm-ocr-api - INFO - 
2025-03-21 10:35:10,152 - olm-ocr-api - INFO - [2025-03-21 10:35:09] Received sigquit from a child process. It usually means the child failed.
2025-03-21 10:35:10,472 - olm-ocr-api - WARNING - Attempt 1: Please wait for sglang server to become ready...
2025-03-21 10:35:11,519 - olm-ocr-api - WARNING - Attempt 2: Please wait for sglang server to become ready...
2025-03-21 10:35:12,562 - olm-ocr-api - WARNING - Attempt 3: Please wait for sglang server to become ready...
2025-03-21 10:35:13,593 - olm-ocr-api - WARNING - Attempt 4: Please wait for sglang server to become ready...
2025-03-21 10:35:14,646 - olm-ocr-api - WARNING - Attempt 5: Please wait for sglang server to become ready...
2025-03-21 10:35:15,678 - olm-ocr-api - WARNING - Attempt 6: Please wait for sglang server to become ready...
2025-03-21 10:41:26,571 - olm-ocr-api - INFO - /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
2025-03-21 10:41:26,571 - olm-ocr-api - INFO -   warnings.warn(
2025-03-21 10:41:29,037 - olm-ocr-api - INFO - INFO 03-21 10:41:29 __init__.py:190] Automatically detected platform cuda.
2025-03-21 10:41:32,055 - olm-ocr-api - INFO - [2025-03-21 10:41:32] server_args=ServerArgs(model_path='allenai/olmOCR-7B-0225-preview', tokenizer_path='allenai/olmOCR-7B-0225-preview', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='allenai/olmOCR-7B-0225-preview', chat_template='qwen2-vl', is_embedding=False, revision=None, host='127.0.0.1', port=30024, mem_fraction_static=0.7, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, stream_interval=1, stream_output=False, random_seed=705059684, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http='warning', log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=5, speculative_eagle_topk=4, speculative_num_draft_tokens=8, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, enable_flashinfer_mla=False, flashinfer_mla_disable_ragged=False, warmups=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False)
2025-03-21 10:41:33,726 - olm-ocr-api - INFO - [2025-03-21 10:41:33] Use chat template for the OpenAI-compatible API server: qwen2-vl
2025-03-21 10:41:35,244 - olm-ocr-api - INFO - /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
2025-03-21 10:41:35,244 - olm-ocr-api - INFO -   warnings.warn(
2025-03-21 10:41:35,274 - olm-ocr-api - INFO - /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
2025-03-21 10:41:35,274 - olm-ocr-api - INFO -   warnings.warn(
2025-03-21 10:41:38,007 - olm-ocr-api - INFO - INFO 03-21 10:41:38 __init__.py:190] Automatically detected platform cuda.
2025-03-21 10:41:38,008 - olm-ocr-api - INFO - INFO 03-21 10:41:38 __init__.py:190] Automatically detected platform cuda.
2025-03-21 10:41:42,160 - olm-ocr-api - INFO - [2025-03-21 10:41:42 TP0] Overlap scheduler is disabled for multimodal models.
2025-03-21 10:41:42,201 - olm-ocr-api - INFO - [2025-03-21 10:41:42 TP0] Automatically reduce --mem-fraction-static to 0.665 because this is a multimodal model.
2025-03-21 10:41:42,201 - olm-ocr-api - INFO - [2025-03-21 10:41:42 TP0] Automatically turn off --chunked-prefill-size and disable radix cache for qwen2-vl.
2025-03-21 10:41:42,202 - olm-ocr-api - INFO - [2025-03-21 10:41:42 TP0] Init torch distributed begin.
2025-03-21 10:41:42,582 - olm-ocr-api - INFO - [2025-03-21 10:41:42 TP0] Init torch distributed ends. mem usage=0.00 GB
2025-03-21 10:41:42,582 - olm-ocr-api - INFO - [2025-03-21 10:41:42 TP0] Load weight begin. avail mem=30.84 GB
2025-03-21 10:41:42,664 - olm-ocr-api - INFO - [2025-03-21 10:41:42 TP0] The following error message 'operation scheduled before its operands' can be ignored.
2025-03-21 10:41:43,170 - olm-ocr-api - INFO - [2025-03-21 10:41:43 TP0] Using model weights format ['*.safetensors']
2025-03-21 10:41:43,253 - olm-ocr-api - INFO - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
2025-03-21 10:41:47,078 - olm-ocr-api - INFO - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.82s/it]
2025-03-21 10:41:48,327 - olm-ocr-api - INFO - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:04,  2.31s/it]
2025-03-21 10:41:50,629 - olm-ocr-api - INFO - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.31s/it]
2025-03-21 10:41:51,500 - olm-ocr-api - INFO - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:08<00:00,  1.74s/it]
2025-03-21 10:41:51,500 - olm-ocr-api - INFO - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:08<00:00,  2.06s/it]
2025-03-21 10:41:51,500 - olm-ocr-api - INFO - 
2025-03-21 10:41:51,584 - olm-ocr-api - INFO - [2025-03-21 10:41:51 TP0] Load weight end. type=Qwen2VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=15.12 GB, mem usage=15.72 GB.
2025-03-21 10:41:52,202 - olm-ocr-api - INFO - [2025-03-21 10:41:52 TP0] KV Cache is allocated. #tokens: 89666, K size: 2.39 GB, V size: 2.39 GB
2025-03-21 10:41:52,202 - olm-ocr-api - INFO - [2025-03-21 10:41:52 TP0] Memory pool end. avail mem=10.04 GB
2025-03-21 10:41:52,411 - olm-ocr-api - INFO - 2025-03-21 10:41:52,411 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend
2025-03-21 10:41:55,210 - olm-ocr-api - INFO - [2025-03-21 10:41:55 TP0] max_total_num_tokens=89666, chunked_prefill_size=-1, max_prefill_tokens=16384, max_running_requests=2049, context_len=32768
2025-03-21 10:41:56,488 - olm-ocr-api - INFO - [2025-03-21 10:41:56 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
2025-03-21 10:41:56,488 - olm-ocr-api - INFO - sglang running req: 0 queue req: 0
2025-03-21 10:41:57,753 - olm-ocr-api - INFO - 2025-03-21 10:41:57,753 - INFO - flashinfer.jit: Loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False
2025-03-21 10:41:59,483 - olm-ocr-api - INFO - [2025-03-21 10:41:59 TP0] Scheduler hit an exception: Traceback (most recent call last):
2025-03-21 10:41:59,483 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2104, in _run_ninja_build
2025-03-21 10:41:59,483 - olm-ocr-api - INFO -     subprocess.run(
2025-03-21 10:41:59,483 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/subprocess.py", line 571, in run
2025-03-21 10:41:59,483 - olm-ocr-api - INFO -     raise CalledProcessError(retcode, process.args,
2025-03-21 10:41:59,483 - olm-ocr-api - INFO - subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.
2025-03-21 10:41:59,484 - olm-ocr-api - INFO - 
2025-03-21 10:41:59,484 - olm-ocr-api - INFO - The above exception was the direct cause of the following exception:
2025-03-21 10:41:59,484 - olm-ocr-api - INFO - 
2025-03-21 10:41:59,484 - olm-ocr-api - INFO - Traceback (most recent call last):
2025-03-21 10:41:59,484 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 1759, in run_scheduler_process
2025-03-21 10:41:59,484 - olm-ocr-api - INFO -     scheduler.event_loop_normal()
2025-03-21 10:41:59,484 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
2025-03-21 10:41:59,484 - olm-ocr-api - INFO -     return func(*args, **kwargs)
2025-03-21 10:41:59,484 - olm-ocr-api - INFO -            ^^^^^^^^^^^^^^^^^^^^^
2025-03-21 10:41:59,484 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 492, in event_loop_normal
2025-03-21 10:41:59,484 - olm-ocr-api - INFO -     result = self.run_batch(batch)
2025-03-21 10:41:59,484 - olm-ocr-api - INFO -              ^^^^^^^^^^^^^^^^^^^^^
2025-03-21 10:41:59,484 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 1196, in run_batch
2025-03-21 10:41:59,484 - olm-ocr-api - INFO -     logits_output, next_token_ids = self.tp_worker.forward_batch_generation(
2025-03-21 10:41:59,484 - olm-ocr-api - INFO -                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-03-21 10:41:59,484 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py", line 172, in forward_batch_generation
2025-03-21 10:41:59,484 - olm-ocr-api - INFO -     logits_output = self.model_runner.forward(forward_batch)
2025-03-21 10:41:59,484 - olm-ocr-api - INFO -                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-03-21 10:41:59,484 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py", line 948, in forward
2025-03-21 10:41:59,484 - olm-ocr-api - INFO -     return self.forward_extend(
2025-03-21 10:41:59,484 - olm-ocr-api - INFO -            ^^^^^^^^^^^^^^^^^^^^
2025-03-21 10:41:59,484 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py", line 905, in forward_extend
2025-03-21 10:41:59,484 - olm-ocr-api - INFO -     self.attn_backend.init_forward_metadata(forward_batch)
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/layers/attention/flashinfer_backend.py", line 237, in init_forward_metadata
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -     self.indices_updater_prefill.update(
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/layers/attention/flashinfer_backend.py", line 740, in update_single_wrapper
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -     self.call_begin_forward(
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/sglang/srt/layers/attention/flashinfer_backend.py", line 897, in call_begin_forward
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -     wrapper_paged.begin_forward(
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/prefill.py", line 1421, in plan
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -     self._cached_module = get_batch_prefill_module(self._backend)(
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/prefill.py", line 197, in backend_module
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -     module = gen_batch_prefill_module(backend, *args)
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/jit/attention/pytorch.py", line 568, in gen_batch_prefill_module
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -     return gen_customize_batch_prefill_module(
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/jit/attention/pytorch.py", line 1012, in gen_customize_batch_prefill_module
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -     return load_cuda_ops(
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -            ^^^^^^^^^^^^^^
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/jit/core.py", line 123, in load_cuda_ops
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -     torch_cpp_ext.load(
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1314, in load
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -     return _jit_compile(
2025-03-21 10:41:59,485 - olm-ocr-api - INFO -            ^^^^^^^^^^^^^
2025-03-21 10:41:59,486 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1721, in _jit_compile
2025-03-21 10:41:59,486 - olm-ocr-api - INFO -     _write_ninja_file_and_build_library(
2025-03-21 10:41:59,486 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1833, in _write_ninja_file_and_build_library
2025-03-21 10:41:59,486 - olm-ocr-api - INFO -     _run_ninja_build(
2025-03-21 10:41:59,486 - olm-ocr-api - INFO -   File "/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2120, in _run_ninja_build
2025-03-21 10:41:59,486 - olm-ocr-api - INFO -     raise RuntimeError(message) from e
2025-03-21 10:41:59,486 - olm-ocr-api - INFO - RuntimeError: Error building extension 'batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False': [1/3] /local/home/hfurquan/myProjects/CUDA/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_prefill_jit_pybind.cuda.o.d -DTORCH_EXTENSION_NAME=batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/TH -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/THC -isystem /local/home/hfurquan/myProjects/CUDA/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 --expt-relaxed-constexpr -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -std=c++17 --threads 4 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -c /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu -o batch_prefill_jit_pybind.cuda.o
2025-03-21 10:41:59,486 - olm-ocr-api - INFO - FAILED: batch_prefill_jit_pybind.cuda.o
2025-03-21 10:41:59,486 - olm-ocr-api - INFO - /local/home/hfurquan/myProjects/CUDA/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_prefill_jit_pybind.cuda.o.d -DTORCH_EXTENSION_NAME=batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/TH -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/THC -isystem /local/home/hfurquan/myProjects/CUDA/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 --expt-relaxed-constexpr -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -std=c++17 --threads 4 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -c /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu -o batch_prefill_jit_pybind.cuda.o
2025-03-21 10:41:59,486 - olm-ocr-api - INFO - In file included from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBase.h:14,
2025-03-21 10:41:59,486 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBody.h:38,
2025-03-21 10:41:59,486 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/jit_type.h:5,
2025-03-21 10:41:59,486 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/function_schema.h:6,
2025-03-21 10:41:59,486 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/op_registration/infer_schema.h:8,
2025-03-21 10:41:59,486 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/library.h:61,
2025-03-21 10:41:59,486 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc/pytorch_extension_utils.h:19,
2025-03-21 10:41:59,486 - olm-ocr-api - INFO -                  from /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu:17:
2025-03-21 10:41:59,486 - olm-ocr-api - INFO - /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/c10/util/C++17.h:13:2: error: #error "You're trying to build PyTorch with a too old version of GCC. We need GCC 9 or later."
2025-03-21 10:41:59,486 - olm-ocr-api - INFO -  #error \
2025-03-21 10:41:59,486 - olm-ocr-api - INFO -   ^~~~~
2025-03-21 10:41:59,486 - olm-ocr-api - INFO - In file included from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBase.h:14,
2025-03-21 10:41:59,486 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBody.h:38,
2025-03-21 10:41:59,487 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/jit_type.h:5,
2025-03-21 10:41:59,487 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/function_schema.h:6,
2025-03-21 10:41:59,487 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/op_registration/infer_schema.h:8,
2025-03-21 10:41:59,487 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/library.h:61,
2025-03-21 10:41:59,487 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc/pytorch_extension_utils.h:19,
2025-03-21 10:41:59,487 - olm-ocr-api - INFO -                  from /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu:17:
2025-03-21 10:41:59,487 - olm-ocr-api - INFO - /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/c10/util/C++17.h:13:2: error: #error "You're trying to build PyTorch with a too old version of GCC. We need GCC 9 or later."
2025-03-21 10:41:59,487 - olm-ocr-api - INFO -  #error \
2025-03-21 10:41:59,487 - olm-ocr-api - INFO -   ^~~~~
2025-03-21 10:41:59,487 - olm-ocr-api - INFO - fatal   : Could not open input file /tmp/tmpxft_0034dcbc_00000000-7_batch_prefill_jit_pybind.cpp1.ii
2025-03-21 10:41:59,487 - olm-ocr-api - INFO - [2/3] /local/home/hfurquan/myProjects/CUDA/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_prefill.cuda.o.d -DTORCH_EXTENSION_NAME=batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/TH -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/THC -isystem /local/home/hfurquan/myProjects/CUDA/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 --expt-relaxed-constexpr -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -std=c++17 --threads 4 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -c /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cu -o batch_prefill.cuda.o
2025-03-21 10:41:59,487 - olm-ocr-api - INFO - FAILED: batch_prefill.cuda.o
2025-03-21 10:41:59,487 - olm-ocr-api - INFO - /local/home/hfurquan/myProjects/CUDA/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_prefill.cuda.o.d -DTORCH_EXTENSION_NAME=batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/include -I/local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/TH -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/THC -isystem /local/home/hfurquan/myProjects/CUDA/include -isystem /local/home/hfurquan/miniconda3/envs/dialectic-bias/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 --expt-relaxed-constexpr -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -std=c++17 --threads 4 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -c /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cu -o batch_prefill.cuda.o
2025-03-21 10:41:59,487 - olm-ocr-api - INFO - In file included from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBase.h:14,
2025-03-21 10:41:59,487 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBody.h:38,
2025-03-21 10:41:59,487 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/jit_type.h:5,
2025-03-21 10:41:59,487 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/function_schema.h:6,
2025-03-21 10:41:59,487 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/op_registration/infer_schema.h:8,
2025-03-21 10:41:59,487 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/library.h:61,
2025-03-21 10:41:59,487 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc/pytorch_extension_utils.h:19,
2025-03-21 10:41:59,487 - olm-ocr-api - INFO -                  from /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cu:22:
2025-03-21 10:41:59,487 - olm-ocr-api - INFO - /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/c10/util/C++17.h:13:2: error: #error "You're trying to build PyTorch with a too old version of GCC. We need GCC 9 or later."
2025-03-21 10:41:59,488 - olm-ocr-api - INFO -  #error \
2025-03-21 10:41:59,488 - olm-ocr-api - INFO -   ^~~~~
2025-03-21 10:41:59,488 - olm-ocr-api - INFO - In file included from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBase.h:14,
2025-03-21 10:41:59,488 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/TensorBody.h:38,
2025-03-21 10:41:59,488 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/jit_type.h:5,
2025-03-21 10:41:59,488 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/function_schema.h:6,
2025-03-21 10:41:59,488 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/ATen/core/op_registration/infer_schema.h:8,
2025-03-21 10:41:59,488 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/torch/library.h:61,
2025-03-21 10:41:59,488 - olm-ocr-api - INFO -                  from /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/flashinfer/data/csrc/pytorch_extension_utils.h:19,
2025-03-21 10:41:59,488 - olm-ocr-api - INFO -                  from /cs/home/hfurquan/.cache/flashinfer/86/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cu:22:
2025-03-21 10:41:59,488 - olm-ocr-api - INFO - /local/home/hfurquan/miniconda3/envs/dialectic-bias/lib/python3.11/site-packages/torch/include/c10/util/C++17.h:13:2: error: #error "You're trying to build PyTorch with a too old version of GCC. We need GCC 9 or later."
2025-03-21 10:41:59,488 - olm-ocr-api - INFO -  #error \
2025-03-21 10:41:59,488 - olm-ocr-api - INFO -   ^~~~~
2025-03-21 10:41:59,488 - olm-ocr-api - INFO - fatal   : Could not open input file /tmp/tmpxft_0034dcbb_00000000-7_batch_prefill.cpp1.ii
2025-03-21 10:41:59,488 - olm-ocr-api - INFO - ninja: build stopped: subcommand failed.
2025-03-21 10:41:59,488 - olm-ocr-api - INFO - 
2025-03-21 10:41:59,488 - olm-ocr-api - INFO - 
2025-03-21 10:41:59,489 - olm-ocr-api - INFO - [2025-03-21 10:41:59] Received sigquit from a child process. It usually means the child failed.
2025-03-21 10:42:00,796 - olm-ocr-api - WARNING - Attempt 1: Please wait for sglang server to become ready...
2025-03-21 10:42:01,824 - olm-ocr-api - WARNING - Attempt 2: Please wait for sglang server to become ready...
2025-03-21 10:42:02,850 - olm-ocr-api - WARNING - Attempt 3: Please wait for sglang server to become ready...
2025-03-21 10:42:03,888 - olm-ocr-api - WARNING - Attempt 4: Please wait for sglang server to become ready...
2025-03-21 10:42:04,915 - olm-ocr-api - WARNING - Attempt 5: Please wait for sglang server to become ready...
2025-03-21 10:42:05,945 - olm-ocr-api - WARNING - Attempt 6: Please wait for sglang server to become ready...
2025-03-21 10:42:06,972 - olm-ocr-api - WARNING - Attempt 7: Please wait for sglang server to become ready...
